{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([\n",
    "    [0.43, 0.15, 0.89], # your\n",
    "    [0.55, 0.87, 0.66], # journey\n",
    "    [0.57, 0.85, 0.64], # starts\n",
    "    [0.22, 0.58, 0.33], # with\n",
    "    [0.77, 0.25, 0.10], # one \n",
    "    [0.05, 0.80, 0.55], # step\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAtention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys  = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @keys.T        \n",
    "        attn_weights = torch.softmax(attn_scores / (self.d_out ** 0.5), dim=-1)\n",
    "        print(f\"attn_weights: \\n{attn_weights}\")\n",
    "            \n",
    "        context_vec  = attn_weights @ values\n",
    "\n",
    "        return context_vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_weights: \n",
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAtention_v2(d_in=3, d_out=2)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAtention_v3(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        keys  = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @keys.T\n",
    "        print(f\"attn_score: \\n{attn_scores}\")\n",
    "        \n",
    "        context_length = attn_scores.shape[-1]\n",
    "        # torch.triu 获得上三角矩阵的mask， diagnoal=1表示向右移动一个元素取上三角\n",
    "        mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        print(f\"mask: \\n{mask}\")\n",
    "        \n",
    "        # masked_fill 将 bool 为 true 的位置填充为 -inf \n",
    "        masked_scores = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "        print(f\"masked: \\n{masked_scores}\")\n",
    "        \n",
    "        # 计算 masked_scores 的 softmax\n",
    "        attn_weights = torch.softmax(masked_scores / (self.d_out ** 0.5), dim=-1)\n",
    "        print(f\"attn_weights: \\n{attn_weights}\")\n",
    "        \n",
    "        # dropout 用于丢弃某些参数，防止模型过于依赖某些部分的参数\n",
    "        # 但是目前的 llm 一般不使用 dropout\n",
    "        torch.manual_seed(123)\n",
    "        # 由于 dropout 50% 的参数，会对剩余的元素进行补偿 1 / (1 - drop_ratio) = 2\n",
    "        dropout = nn.Dropout(p=0.5)\n",
    "        attn_weights = dropout(attn_weights)\n",
    "        print(f\"attn_weights after dropout: \\n{attn_weights}\")\n",
    "            \n",
    "        context_vec  = attn_weights @ values\n",
    "\n",
    "        return context_vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_score: \n",
      "tensor([[ 0.2899,  0.0716,  0.0760, -0.0138,  0.1344, -0.0511],\n",
      "        [ 0.4656,  0.1723,  0.1751,  0.0259,  0.1771,  0.0085],\n",
      "        [ 0.4594,  0.1703,  0.1731,  0.0259,  0.1745,  0.0090],\n",
      "        [ 0.2642,  0.1024,  0.1036,  0.0186,  0.0973,  0.0122],\n",
      "        [ 0.2183,  0.0874,  0.0882,  0.0177,  0.0786,  0.0144],\n",
      "        [ 0.3408,  0.1270,  0.1290,  0.0198,  0.1290,  0.0078]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "mask: \n",
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "masked: \n",
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "attn_weights: \n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "attn_weights after dropout: \n",
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "tensor([[-0.1744,  0.0572],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [-0.1999,  0.1267],\n",
      "        [-0.1061,  0.0833],\n",
      "        [-0.0795,  0.0294],\n",
      "        [-0.0534,  0.1748]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v3 = SelfAtention_v3(d_in=3, d_out=2)\n",
    "print(sa_v3(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # register_buffer 可以在使用 gpu 时，将变量自动转移到 gpu 上\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        # 这里由于存在 batch, keys and queries 的形状是 (b, num_tokens, d_out)\n",
    "        # 所以 keys 的转置为 (num_tokens, d_out) 即 1 - 2 维\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / (self.d_out ** 0.5), dim=-1)\n",
    "        # print(f\"attn_weights: \\n{attn_weights}\")\n",
    "        \n",
    "        # 这里在 attn_weight 应用 dropout\n",
    "        # 这里为什么不在 attn_scores 上应用 dropout 呢？\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        # print(f\"attn_weights after dropout: \\n{attn_weights}\")\n",
    "        \n",
    "        context_vec = attn_weights @ values\n",
    "        \n",
    "        return context_vec\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch shape: torch.Size([2, 6, 3])\n",
      "attn_weights: \n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
      "         [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
      "         [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
      "         [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
      "         [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "attn_weights after dropout: \n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
      "         [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
      "         [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
      "         [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
      "         [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "context_vecs shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim = 0)\n",
    "print(f\"batch shape: {batch.shape}\")\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in = 3, d_out=2, context_length = context_length, dropout=0.0, qkv_bias=False)\n",
    "context_vecs = ca(batch)\n",
    "print(f\"context_vecs shape: {context_vecs.shape}\")\n",
    "printf(f\"context_vecs: \\n{context_vecs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs shape: torch.Size([2, 6, 2])\n",
      "context_vecs: tensor([[[-0.5740,  0.2216],\n",
      "         [-0.7320,  0.0155],\n",
      "         [-0.7774, -0.0546],\n",
      "         [-0.6979, -0.0817],\n",
      "         [-0.6538, -0.0957],\n",
      "         [-0.6424, -0.1065]],\n",
      "\n",
      "        [[-0.5740,  0.2216],\n",
      "         [-0.7320,  0.0155],\n",
      "         [-0.7774, -0.0546],\n",
      "         [-0.6979, -0.0817],\n",
      "         [-0.6538, -0.0957],\n",
      "         [-0.6424, -0.1065]]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # 一个 batch 中的 token 数量\n",
    "d_in, d_out = 3, 1 # 输入和输出的维度\n",
    "hma = MultiHeadAttentionWrapper(\n",
    "    d_in = d_in,\n",
    "    d_out = d_out,\n",
    "    context_length = context_length,\n",
    "    dropout = 0.0,\n",
    "    num_heads = 2\n",
    ")\n",
    "context_vecs = hma(batch)\n",
    "print(f\"context_vecs shape: {context_vecs.shape}\")\n",
    "print(f\"context_vecs: {context_vecs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        # 每个头的维度\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "        # 使用一个 projection 层将所有头的输出合并\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.register_buffer(\n",
    "            'mask',\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        # 将 d_out 展开为 (num_heads, head_dim)\n",
    "        # 最终得到维度 (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # 转置使得维度变为 (b, num_heads, num_tokens, head_dim)\n",
    "        # (num_heads, head_dim) 可以按照原来方法计算\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "        \n",
    "        # 计算每个头的注意力分数\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask_pool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_pool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / (self.head_dim ** 0.5), dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # attn_weight @ values 的结果维度为 (b, num_heads, num_tokens, head_dim)\n",
    "        # 通过转置，得到 (b, num_tokens, num_heads, head_dim)\n",
    "        context_vecs = (attn_weights @ values).transpose(1, 2)\n",
    "        # 将 context_vecs 的维度从 (b, num_heads, num_tokens, head_dim) 转换为 (b, num_tokens, d_out)\n",
    "        context_vecs = context_vecs.contiguous().view(b, num_tokens, self.d_out)\n",
    "        \n",
    "        # 添加一层 linear projection 变换，但保证输出维度仍然是 (b, num_tokens, d_out)\n",
    "        context_vecs = self.out_proj(context_vecs)\n",
    "        \n",
    "        return context_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch shape: torch.Size([2, 6, 3])\n",
      "context_vecs shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([\n",
    "    [0.43, 0.15, 0.89], # your\n",
    "    [0.55, 0.87, 0.66], # journey\n",
    "    [0.57, 0.85, 0.64], # starts\n",
    "    [0.22, 0.58, 0.33], # with\n",
    "    [0.77, 0.25, 0.10], # one \n",
    "    [0.05, 0.80, 0.55], # step\n",
    "])\n",
    "\n",
    "\n",
    "batch = torch.stack((inputs, inputs), dim = 0)\n",
    "print(f\"batch shape: {batch.shape}\")\n",
    "\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "mha = MultiHeadAttention(d_in = 3, d_out=2, context_length = context_length, dropout=0.0, num_heads=2, qkv_bias=False)\n",
    "context_vecs = mha(batch)\n",
    "print(f\"context_vecs shape: {context_vecs.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
